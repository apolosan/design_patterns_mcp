{
  "patterns": [
    {
      "id": "model-versioning",
      "name": "Model Versioning Pattern",
      "category": "MLOps",
      "description": "Systematically manages multiple versions of ML models in production, enabling rollback, A/B testing, and gradual rollout strategies",
      "problem": "Managing multiple model versions, tracking performance, and enabling safe deployments is complex without systematic versioning",
      "solution": "Implement version control for models with metadata tracking, semantic versioning, and deployment strategies (blue-green, canary, shadow)",
      "when_to_use": [
        "Multiple models in production",
        "Need to rollback deployments",
        "A/B testing requirements",
        "Compliance and audit trails needed"
      ],
      "benefits": [
        "Safe deployments",
        "Easy rollback capability",
        "Experiment tracking",
        "Audit trail",
        "Gradual rollout support"
      ],
      "drawbacks": [
        "Storage overhead",
        "Versioning complexity",
        "Metadata management burden",
        "Potential consistency issues"
      ],
      "use_cases": [
        "Production ML services",
        "Recommendation systems",
        "Fraud detection models",
        "NLP model deployments",
        "Computer vision applications"
      ],
      "related_patterns": ["feature-engineering-pipeline", "model-versioning", "CI/CD Pipeline"],
      "complexity": "High",
      "tags": ["mlops", "versioning", "deployment", "model-management", "production"],
      "code_examples": [
        {
          "language": "python",
          "code": "import mlflow\nimport mlflow.sklearn\nfrom datetime import datetime\nimport json\n\nclass ModelVersionManager:\n    \"\"\"Manage ML model versions with MLflow\"\"\"\n    \n    def __init__(self, tracking_uri: str = \"http://localhost:5000\"):\n        mlflow.set_tracking_uri(tracking_uri)\n        self.client = mlflow.tracking.MlflowClient()\n    \n    def register_model(\n        self,\n        model,\n        model_name: str,\n        metrics: dict,\n        parameters: dict,\n        tags: dict = None\n    ) -> str:\n        \"\"\"Register new model version\"\"\"\n        with mlflow.start_run() as run:\n            # Log model\n            mlflow.sklearn.log_model(model, \"model\")\n            \n            # Log metrics\n            for metric_name, value in metrics.items():\n                mlflow.log_metric(metric_name, value)\n            \n            # Log parameters\n            for param_name, value in parameters.items():\n                mlflow.log_param(param_name, value)\n            \n            # Log tags\n            if tags:\n                mlflow.set_tags(tags)\n            \n            # Register model\n            model_uri = f\"runs:/{run.info.run_id}/model\"\n            model_version = mlflow.register_model(model_uri, model_name)\n            \n            return model_version.version\n    \n    def promote_model(self, model_name: str, version: str, stage: str):\n        \"\"\"Promote model version to stage (Staging, Production, Archived)\"\"\"\n        self.client.transition_model_version_stage(\n            name=model_name,\n            version=version,\n            stage=stage\n        )\n        print(f\"Model {model_name} v{version} promoted to {stage}\")\n    \n    def get_production_model(self, model_name: str):\n        \"\"\"Get current production model\"\"\"\n        model_uri = f\"models:/{model_name}/Production\"\n        return mlflow.sklearn.load_model(model_uri)\n    \n    def compare_versions(self, model_name: str, versions: list):\n        \"\"\"Compare metrics across model versions\"\"\"\n        comparison = []\n        \n        for version in versions:\n            model_version = self.client.get_model_version(model_name, version)\n            run = self.client.get_run(model_version.run_id)\n            \n            comparison.append({\n                'version': version,\n                'stage': model_version.current_stage,\n                'metrics': run.data.metrics,\n                'created_at': model_version.creation_timestamp\n            })\n        \n        return comparison\n    \n    def rollback_to_version(self, model_name: str, version: str):\n        \"\"\"Rollback to previous version\"\"\"\n        # Demote current production\n        prod_versions = self.client.get_latest_versions(model_name, stages=[\"Production\"])\n        for pv in prod_versions:\n            self.client.transition_model_version_stage(\n                name=model_name,\n                version=pv.version,\n                stage=\"Archived\"\n            )\n        \n        # Promote target version\n        self.promote_model(model_name, version, \"Production\")\n        print(f\"Rolled back to version {version}\")\n\n# Usage\nmanager = ModelVersionManager()\n\n# Train and register new model\nmodel = train_model(X_train, y_train)\nmetrics = {'accuracy': 0.95, 'f1_score': 0.93}\nparameters = {'n_estimators': 100, 'max_depth': 10}\n\nversion = manager.register_model(\n    model,\n    model_name='fraud_detector',\n    metrics=metrics,\n    parameters=parameters,\n    tags={'team': 'ml-team', 'project': 'fraud-detection'}\n)\n\nprint(f\"Registered model version: {version}\")\n\n# Promote to production\nmanager.promote_model('fraud_detector', version, 'Production')\n\n# Load production model\nprod_model = manager.get_production_model('fraud_detector')\n\n# Compare versions\ncomparison = manager.compare_versions('fraud_detector', ['1', '2', '3'])\nfor v in comparison:\n    print(f\"Version {v['version']}: Accuracy = {v['metrics'].get('accuracy', 'N/A')}\")"
        },
        {
          "language": "python",
          "code": "# Custom model versioning system\nimport hashlib\nimport pickle\nfrom pathlib import Path\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, Any\nimport json\nfrom datetime import datetime\n\n@dataclass\nclass ModelMetadata:\n    version: str\n    created_at: str\n    model_type: str\n    metrics: Dict[str, float]\n    parameters: Dict[str, Any]\n    training_data_hash: str\n    model_hash: str\n    tags: Dict[str, str]\n    stage: str = \"development\"  # development, staging, production\n\nclass ModelRegistry:\n    \"\"\"Custom model registry with versioning\"\"\"\n    \n    def __init__(self, registry_path: str = \"./model_registry\"):\n        self.registry_path = Path(registry_path)\n        self.registry_path.mkdir(parents=True, exist_ok=True)\n        self.metadata_file = self.registry_path / \"metadata.json\"\n        self.metadata = self._load_metadata()\n    \n    def _load_metadata(self) -> dict:\n        if self.metadata_file.exists():\n            with open(self.metadata_file, 'r') as f:\n                return json.load(f)\n        return {}\n    \n    def _save_metadata(self):\n        with open(self.metadata_file, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n    \n    def _compute_model_hash(self, model) -> str:\n        \"\"\"Compute hash of model for versioning\"\"\"\n        model_bytes = pickle.dumps(model)\n        return hashlib.sha256(model_bytes).hexdigest()[:16]\n    \n    def register(\n        self,\n        model_name: str,\n        model: Any,\n        metrics: Dict[str, float],\n        parameters: Dict[str, Any],\n        training_data_hash: str = None,\n        tags: Dict[str, str] = None\n    ) -> str:\n        \"\"\"Register new model version\"\"\"\n        # Generate version\n        model_hash = self._compute_model_hash(model)\n        existing_versions = self.metadata.get(model_name, {})\n        version = f\"v{len(existing_versions) + 1}\"\n        \n        # Create metadata\n        metadata = ModelMetadata(\n            version=version,\n            created_at=datetime.now().isoformat(),\n            model_type=type(model).__name__,\n            metrics=metrics,\n            parameters=parameters,\n            training_data_hash=training_data_hash or \"unknown\",\n            model_hash=model_hash,\n            tags=tags or {},\n            stage=\"development\"\n        )\n        \n        # Save model file\n        model_path = self.registry_path / model_name / version\n        model_path.mkdir(parents=True, exist_ok=True)\n        \n        with open(model_path / \"model.pkl\", 'wb') as f:\n            pickle.dump(model, f)\n        \n        with open(model_path / \"metadata.json\", 'w') as f:\n            json.dump(asdict(metadata), f, indent=2)\n        \n        # Update registry metadata\n        if model_name not in self.metadata:\n            self.metadata[model_name] = {}\n        self.metadata[model_name][version] = asdict(metadata)\n        self._save_metadata()\n        \n        return version\n    \n    def load(self, model_name: str, version: str = None, stage: str = None):\n        \"\"\"Load model by version or stage\"\"\"\n        if stage:\n            # Find version with specified stage\n            for v, meta in self.metadata.get(model_name, {}).items():\n                if meta['stage'] == stage:\n                    version = v\n                    break\n        \n        if not version:\n            raise ValueError(f\"No model found for {model_name} with stage={stage}\")\n        \n        model_path = self.registry_path / model_name / version / \"model.pkl\"\n        with open(model_path, 'rb') as f:\n            return pickle.load(f)\n    \n    def set_stage(self, model_name: str, version: str, stage: str):\n        \"\"\"Set model version stage\"\"\"\n        if stage == \"production\":\n            # Demote current production\n            for v, meta in self.metadata.get(model_name, {}).items():\n                if meta['stage'] == 'production':\n                    self.metadata[model_name][v]['stage'] = 'archived'\n        \n        self.metadata[model_name][version]['stage'] = stage\n        self._save_metadata()\n        print(f\"{model_name} {version} → {stage}\")\n    \n    def list_versions(self, model_name: str) -> list:\n        \"\"\"List all versions of a model\"\"\"\n        return list(self.metadata.get(model_name, {}).keys())\n\n# Usage\nregistry = ModelRegistry()\n\n# Register model\nversion = registry.register(\n    model_name='churn_predictor',\n    model=trained_model,\n    metrics={'accuracy': 0.92, 'auc': 0.88},\n    parameters={'n_estimators': 100, 'learning_rate': 0.1},\n    tags={'team': 'analytics', 'priority': 'high'}\n)\n\n# Promote to production\nregistry.set_stage('churn_predictor', version, 'production')\n\n# Load production model\nmodel = registry.load('churn_predictor', stage='production')\n\n# List all versions\nversions = registry.list_versions('churn_predictor')\nprint(f\"Available versions: {versions}\")"
        }
      ]
    },
    {
      "id": "feature-engineering-pipeline",
      "name": "Feature Engineering Pipeline Pattern",
      "category": "Data Engineering",
      "description": "Automates feature transformation and engineering processes in reusable, versioned pipelines that ensure consistency between training and inference",
      "problem": "Manual feature engineering is error-prone, inconsistent between training/inference, and difficult to reproduce",
      "solution": "Create structured, versioned transformation pipelines with consistent feature extraction logic that can be applied identically across training and production",
      "when_to_use": [
        "Complex feature transformations needed",
        "Multiple models share features",
        "Training-serving skew risks",
        "Need reproducible feature engineering"
      ],
      "benefits": [
        "Consistency guarantee",
        "Reusability across models",
        "Version control for features",
        "Reduced training-serving skew",
        "Automated transformations"
      ],
      "drawbacks": [
        "Initial setup complexity",
        "Pipeline maintenance overhead",
        "Debugging challenges",
        "Performance tuning needed"
      ],
      "use_cases": [
        "Time-series feature generation",
        "Text feature extraction",
        "Image preprocessing pipelines",
        "Categorical encoding",
        "Anomaly detection features"
      ],
      "related_patterns": [
        "feature-engineering-pipeline",
        "Data Preprocessing",
        "model-versioning"
      ],
      "complexity": "High",
      "tags": ["feature-engineering", "pipeline", "data-preprocessing", "mlops", "transformation"],
      "code_examples": [
        {
          "language": "python",
          "code": "from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nimport pandas as pd\nimport numpy as np\n\nclass TemporalFeatureExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"Extract temporal features from datetime column\"\"\"\n    \n    def __init__(self, datetime_column='timestamp'):\n        self.datetime_column = datetime_column\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        dt = pd.to_datetime(X[self.datetime_column])\n        \n        X['hour'] = dt.dt.hour\n        X['day_of_week'] = dt.dt.dayofweek\n        X['day_of_month'] = dt.dt.day\n        X['month'] = dt.dt.month\n        X['is_weekend'] = dt.dt.dayofweek.isin([5, 6]).astype(int)\n        X['is_business_hours'] = ((dt.dt.hour >= 9) & (dt.dt.hour < 17)).astype(int)\n        \n        return X.drop(columns=[self.datetime_column])\n\nclass RollingStatsTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Calculate rolling statistics\"\"\"\n    \n    def __init__(self, column, windows=[7, 30]):\n        self.column = column\n        self.windows = windows\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        \n        for window in self.windows:\n            X[f'{self.column}_rolling_mean_{window}'] = X[self.column].rolling(\n                window=window, min_periods=1\n            ).mean()\n            X[f'{self.column}_rolling_std_{window}'] = X[self.column].rolling(\n                window=window, min_periods=1\n            ).std().fillna(0)\n        \n        return X\n\nclass FeatureEngineeringPipeline:\n    \"\"\"Complete feature engineering pipeline\"\"\"\n    \n    def __init__(self):\n        # Define numeric and categorical features\n        self.numeric_features = ['age', 'income', 'transaction_amount']\n        self.categorical_features = ['category', 'region', 'product_type']\n        \n        # Numeric pipeline\n        numeric_transformer = Pipeline(steps=[\n            ('scaler', StandardScaler())\n        ])\n        \n        # Categorical pipeline\n        categorical_transformer = Pipeline(steps=[\n            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n        ])\n        \n        # Combine transformers\n        self.preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', numeric_transformer, self.numeric_features),\n                ('cat', categorical_transformer, self.categorical_features)\n            ],\n            remainder='passthrough'\n        )\n        \n        # Full pipeline\n        self.pipeline = Pipeline([\n            ('temporal', TemporalFeatureExtractor('timestamp')),\n            ('rolling', RollingStatsTransformer('transaction_amount', windows=[7, 30])),\n            ('preprocessor', self.preprocessor)\n        ])\n    \n    def fit(self, X, y=None):\n        self.pipeline.fit(X, y)\n        return self\n    \n    def transform(self, X):\n        return self.pipeline.transform(X)\n    \n    def fit_transform(self, X, y=None):\n        return self.pipeline.fit_transform(X, y)\n    \n    def save(self, path: str):\n        import joblib\n        joblib.dump(self.pipeline, path)\n    \n    @classmethod\n    def load(cls, path: str):\n        import joblib\n        instance = cls()\n        instance.pipeline = joblib.load(path)\n        return instance\n\n# Usage\ndf_train = pd.read_csv('training_data.csv')\ndf_test = pd.read_csv('test_data.csv')\n\n# Create and fit pipeline\nfeature_pipeline = FeatureEngineeringPipeline()\nX_train_transformed = feature_pipeline.fit_transform(df_train)\n\n# Save pipeline for production\nfeature_pipeline.save('feature_pipeline.pkl')\n\n# In production\nproduction_pipeline = FeatureEngineeringPipeline.load('feature_pipeline.pkl')\nX_test_transformed = production_pipeline.transform(df_test)\n\nprint(f\"Original features: {df_train.shape[1]}\")\nprint(f\"Engineered features: {X_train_transformed.shape[1]}\")"
        },
        {
          "language": "python",
          "code": "# Advanced feature engineering with custom transformers\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Callable\n\nclass FeatureStore:\n    \"\"\"Centralized feature engineering and storage\"\"\"\n    \n    def __init__(self):\n        self.features = {}\n        self.transformers = []\n    \n    def add_transformer(self, name: str, func: Callable):\n        \"\"\"Add feature transformation function\"\"\"\n        self.transformers.append((name, func))\n    \n    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply all registered transformers\"\"\"\n        result = df.copy()\n        \n        for name, func in self.transformers:\n            try:\n                result = func(result)\n                self.features[name] = True\n                print(f\"✓ Applied {name}\")\n            except Exception as e:\n                print(f\"✗ Failed {name}: {e}\")\n                self.features[name] = False\n        \n        return result\n    \n    def get_feature_names(self) -> List[str]:\n        return list(self.features.keys())\n\n# Define feature engineering functions\ndef create_interaction_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Create interaction features\"\"\"\n    df['price_per_quantity'] = df['price'] / (df['quantity'] + 1)\n    df['revenue'] = df['price'] * df['quantity']\n    return df\n\ndef create_aggregation_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Create aggregated features by customer\"\"\"\n    customer_agg = df.groupby('customer_id').agg({\n        'transaction_amount': ['mean', 'sum', 'count'],\n        'timestamp': 'max'\n    }).reset_index()\n    \n    customer_agg.columns = ['_'.join(col).strip('_') for col in customer_agg.columns]\n    df = df.merge(customer_agg, on='customer_id', how='left', suffixes=('', '_agg'))\n    return df\n\ndef create_text_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Extract features from text\"\"\"\n    if 'description' in df.columns:\n        df['description_length'] = df['description'].str.len()\n        df['word_count'] = df['description'].str.split().str.len()\n        df['has_special_chars'] = df['description'].str.contains(r'[!@#$%^&*]').astype(int)\n    return df\n\ndef create_recency_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Create recency-based features\"\"\"\n    df = df.sort_values('timestamp')\n    df['days_since_last_transaction'] = df.groupby('customer_id')['timestamp'].diff().dt.days\n    df['days_since_last_transaction'].fillna(0, inplace=True)\n    return df\n\n# Usage\nfeature_store = FeatureStore()\n\n# Register transformers\nfeature_store.add_transformer('interaction', create_interaction_features)\nfeature_store.add_transformer('aggregation', create_aggregation_features)\nfeature_store.add_transformer('text', create_text_features)\nfeature_store.add_transformer('recency', create_recency_features)\n\n# Apply all transformations\ndf_raw = pd.read_csv('raw_data.csv')\ndf_engineered = feature_store.engineer_features(df_raw)\n\nprint(f\"\\nOriginal columns: {len(df_raw.columns)}\")\nprint(f\"Engineered columns: {len(df_engineered.columns)}\")\nprint(f\"\\nFeatures created: {feature_store.get_feature_names()}\")"
        }
      ]
    },
    {
      "id": "edge-ai-optimization",
      "name": "Edge AI Optimization Pattern",
      "category": "Edge Computing",
      "description": "Optimizes AI models for deployment on resource-constrained edge devices through quantization, pruning, and knowledge distillation",
      "problem": "Full-sized AI models are too large and computationally expensive for edge devices with limited CPU, memory, and power",
      "solution": "Apply model compression techniques (quantization, pruning, distillation) to reduce model size and inference time while preserving accuracy",
      "when_to_use": [
        "Deploy models on mobile devices",
        "IoT and embedded systems",
        "Real-time edge inference needed",
        "Limited power budget",
        "Network connectivity constraints"
      ],
      "benefits": [
        "Reduced model size (4-10x)",
        "Faster inference",
        "Lower power consumption",
        "Offline capability",
        "Reduced latency"
      ],
      "drawbacks": [
        "Accuracy degradation",
        "Complex optimization process",
        "Hardware-specific tuning",
        "Limited model complexity"
      ],
      "use_cases": [
        "Mobile computer vision",
        "Voice assistants on devices",
        "Smart camera analytics",
        "Autonomous vehicle perception",
        "Industrial IoT predictive maintenance"
      ],
      "related_patterns": ["Knowledge Distillation", "transfer-learning", "Model Compression"],
      "complexity": "Very High",
      "tags": ["edge-computing", "model-optimization", "quantization", "pruning", "mobile-ai"],
      "code_examples": [
        {
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.quantization as quant\n\nclass EdgeOptimizer:\n    \"\"\"Optimize models for edge deployment\"\"\"\n    \n    @staticmethod\n    def quantize_model(model, calibration_data):\n        \"\"\"Post-training static quantization\"\"\"\n        model.eval()\n        \n        # Prepare model for quantization\n        model.qconfig = quant.get_default_qconfig('fbgemm')\n        quant.prepare(model, inplace=True)\n        \n        # Calibrate with representative data\n        with torch.no_grad():\n            for data in calibration_data:\n                model(data)\n        \n        # Convert to quantized model\n        quantized_model = quant.convert(model, inplace=False)\n        return quantized_model\n    \n    @staticmethod\n    def prune_model(model, amount=0.3):\n        \"\"\"Structured pruning to reduce model size\"\"\"\n        import torch.nn.utils.prune as prune\n        \n        parameters_to_prune = []\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                parameters_to_prune.append((module, 'weight'))\n        \n        # Apply global unstructured pruning\n        prune.global_unstructured(\n            parameters_to_prune,\n            pruning_method=prune.L1Unstructured,\n            amount=amount\n        )\n        \n        # Remove pruning reparameterization\n        for module, _ in parameters_to_prune:\n            prune.remove(module, 'weight')\n        \n        return model\n    \n    @staticmethod\n    def export_to_mobile(model, sample_input, output_path):\n        \"\"\"Export model for mobile deployment\"\"\"\n        model.eval()\n        traced_model = torch.jit.trace(model, sample_input)\n        \n        # Optimize for mobile\n        optimized_model = torch.jit.optimize_for_mobile(traced_model)\n        optimized_model._save_for_lite_interpreter(output_path)\n        \n        return output_path\n\nclass KnowledgeDistillation:\n    \"\"\"Train smaller model from larger teacher model\"\"\"\n    \n    def __init__(self, teacher_model, student_model, temperature=3.0):\n        self.teacher = teacher_model\n        self.student = student_model\n        self.temperature = temperature\n        self.teacher.eval()\n    \n    def distillation_loss(self, student_logits, teacher_logits, labels, alpha=0.5):\n        \"\"\"Combined loss: soft targets + hard targets\"\"\"\n        # Soft targets from teacher\n        soft_targets = nn.functional.softmax(teacher_logits / self.temperature, dim=1)\n        soft_student = nn.functional.log_softmax(student_logits / self.temperature, dim=1)\n        distillation_loss = nn.functional.kl_div(\n            soft_student, \n            soft_targets, \n            reduction='batchmean'\n        ) * (self.temperature ** 2)\n        \n        # Hard targets (ground truth)\n        student_loss = nn.functional.cross_entropy(student_logits, labels)\n        \n        # Combined loss\n        return alpha * distillation_loss + (1 - alpha) * student_loss\n    \n    def train_student(self, train_loader, epochs=10, lr=0.001):\n        \"\"\"Train student model\"\"\"\n        optimizer = torch.optim.Adam(self.student.parameters(), lr=lr)\n        \n        for epoch in range(epochs):\n            self.student.train()\n            total_loss = 0\n            \n            for data, labels in train_loader:\n                optimizer.zero_grad()\n                \n                # Get teacher predictions\n                with torch.no_grad():\n                    teacher_logits = self.teacher(data)\n                \n                # Get student predictions\n                student_logits = self.student(data)\n                \n                # Compute distillation loss\n                loss = self.distillation_loss(student_logits, teacher_logits, labels)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n            \n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n        \n        return self.student\n\n# Usage Example\nimport torchvision.models as models\n\n# Load pretrained teacher model\nteacher = models.resnet50(pretrained=True)\n\n# Define smaller student model\nstudent = models.mobilenet_v2(pretrained=False)\n\n# Knowledge Distillation\ndistiller = KnowledgeDistillation(teacher, student, temperature=4.0)\nstudent_model = distiller.train_student(train_loader, epochs=10)\n\n# Post-training optimization\noptimizer = EdgeOptimizer()\n\n# Quantization\ncalibration_data = [next(iter(train_loader))[0] for _ in range(100)]\nquantized_model = optimizer.quantize_model(student_model, calibration_data)\n\n# Pruning\npruned_model = optimizer.prune_model(quantized_model, amount=0.4)\n\n# Export for mobile\nsample_input = torch.randn(1, 3, 224, 224)\noptimizer.export_to_mobile(pruned_model, sample_input, 'model_mobile.pt')\n\nprint(\"Model optimized for edge deployment!\")"
        },
        {
          "language": "python",
          "code": "# TensorFlow Lite optimization\nimport tensorflow as tf\nimport numpy as np\n\nclass TFLiteOptimizer:\n    \"\"\"Optimize TensorFlow models for mobile/edge\"\"\"\n    \n    @staticmethod\n    def convert_to_tflite(\n        model,\n        representative_dataset=None,\n        quantize=True,\n        optimize_for='latency'\n    ):\n        \"\"\"Convert to TensorFlow Lite with optimizations\"\"\"\n        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n        \n        # Optimization settings\n        if optimize_for == 'size':\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        elif optimize_for == 'latency':\n            converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n        \n        # Quantization\n        if quantize and representative_dataset:\n            converter.representative_dataset = representative_dataset\n            converter.target_spec.supported_ops = [\n                tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n            ]\n            converter.inference_input_type = tf.uint8\n            converter.inference_output_type = tf.uint8\n        \n        # Convert\n        tflite_model = converter.convert()\n        return tflite_model\n    \n    @staticmethod\n    def benchmark_model(tflite_model, test_data, num_runs=100):\n        \"\"\"Benchmark TFLite model performance\"\"\"\n        import time\n        \n        interpreter = tf.lite.Interpreter(model_content=tflite_model)\n        interpreter.allocate_tensors()\n        \n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n        \n        # Warmup\n        for _ in range(10):\n            interpreter.set_tensor(input_details[0]['index'], test_data[0:1])\n            interpreter.invoke()\n        \n        # Benchmark\n        times = []\n        for i in range(num_runs):\n            start = time.time()\n            interpreter.set_tensor(input_details[0]['index'], test_data[i:i+1])\n            interpreter.invoke()\n            output = interpreter.get_tensor(output_details[0]['index'])\n            times.append(time.time() - start)\n        \n        return {\n            'avg_latency_ms': np.mean(times) * 1000,\n            'std_latency_ms': np.std(times) * 1000,\n            'min_latency_ms': np.min(times) * 1000,\n            'max_latency_ms': np.max(times) * 1000\n        }\n    \n    @staticmethod\n    def compare_model_sizes(original_model, optimized_model_path):\n        \"\"\"Compare model sizes\"\"\"\n        import os\n        \n        # Save original model temporarily\n        original_model.save('/tmp/original_model.h5')\n        original_size = os.path.getsize('/tmp/original_model.h5')\n        optimized_size = os.path.getsize(optimized_model_path)\n        \n        compression_ratio = original_size / optimized_size\n        \n        return {\n            'original_mb': original_size / (1024 * 1024),\n            'optimized_mb': optimized_size / (1024 * 1024),\n            'compression_ratio': compression_ratio,\n            'size_reduction_percent': (1 - 1/compression_ratio) * 100\n        }\n\n# Usage\nmodel = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=True,\n    weights='imagenet'\n)\n\n# Representative dataset for quantization\ndef representative_dataset_gen():\n    for _ in range(100):\n        yield [np.random.rand(1, 224, 224, 3).astype(np.float32)]\n\noptimizer = TFLiteOptimizer()\n\n# Convert with quantization\ntflite_model = optimizer.convert_to_tflite(\n    model,\n    representative_dataset=representative_dataset_gen,\n    quantize=True,\n    optimize_for='latency'\n)\n\n# Save optimized model\nwith open('model_optimized.tflite', 'wb') as f:\n    f.write(tflite_model)\n\n# Benchmark\ntest_data = np.random.rand(100, 224, 224, 3).astype(np.float32)\nresults = optimizer.benchmark_model(tflite_model, test_data)\nprint(f\"Average latency: {results['avg_latency_ms']:.2f} ms\")\n\n# Compare sizes\nsize_comparison = optimizer.compare_model_sizes(model, 'model_optimized.tflite')\nprint(f\"Size reduction: {size_comparison['size_reduction_percent']:.1f}%\")\nprint(f\"Compression ratio: {size_comparison['compression_ratio']:.1f}x\")"
        }
      ]
    },
    {
      "id": "real-time-inference",
      "name": "Real-Time Inference Pattern",
      "category": "Performance",
      "description": "Optimizes ML inference pipelines for low-latency, high-throughput predictions with caching, batching, and async processing",
      "problem": "ML inference latency is too high for real-time applications, causing poor user experience or missed SLAs",
      "solution": "Implement inference optimizations including request batching, result caching, model optimization, and async processing to minimize latency",
      "when_to_use": [
        "Real-time user-facing applications",
        "High-frequency trading",
        "Online advertising",
        "Gaming AI",
        "Autonomous systems"
      ],
      "benefits": [
        "Low latency (<100ms)",
        "High throughput",
        "Efficient resource usage",
        "Better user experience",
        "Cost optimization"
      ],
      "drawbacks": [
        "Complex implementation",
        "Cache invalidation challenges",
        "Batching delays",
        "Monitoring overhead"
      ],
      "use_cases": [
        "Recommendation engines",
        "Fraud detection",
        "Ad serving",
        "Real-time translation",
        "Game AI opponents"
      ],
      "related_patterns": ["cache-aside", "load-balancing", "auto-scaling", "circuit-breaker"],
      "complexity": "High",
      "tags": ["inference", "latency", "real-time", "performance", "caching"],
      "code_examples": [
        {
          "language": "python",
          "code": "import asyncio\nimport torch\nimport numpy as np\nfrom collections import deque\nimport time\nfrom functools import lru_cache\nimport hashlib\n\nclass BatchedInferenceService:\n    \"\"\"Batched inference for improved throughput\"\"\"\n    \n    def __init__(self, model, max_batch_size=32, max_wait_ms=10):\n        self.model = model\n        self.model.eval()\n        self.max_batch_size = max_batch_size\n        self.max_wait_ms = max_wait_ms / 1000.0  # Convert to seconds\n        \n        self.request_queue = deque()\n        self.processing = False\n        \n        # Start batch processor\n        asyncio.create_task(self._batch_processor())\n    \n    async def predict(self, input_data):\n        \"\"\"Submit prediction request\"\"\"\n        future = asyncio.Future()\n        self.request_queue.append((input_data, future))\n        return await future\n    \n    async def _batch_processor(self):\n        \"\"\"Process requests in batches\"\"\"\n        while True:\n            if not self.request_queue:\n                await asyncio.sleep(0.001)\n                continue\n            \n            batch = []\n            futures = []\n            start_time = time.time()\n            \n            # Collect batch\n            while self.request_queue and len(batch) < self.max_batch_size:\n                data, future = self.request_queue.popleft()\n                batch.append(data)\n                futures.append(future)\n                \n                # Check if max wait time exceeded\n                if time.time() - start_time > self.max_wait_ms:\n                    break\n            \n            # Process batch\n            if batch:\n                try:\n                    batch_tensor = torch.stack(batch)\n                    with torch.no_grad():\n                        predictions = self.model(batch_tensor)\n                    \n                    # Return results\n                    for future, prediction in zip(futures, predictions):\n                        future.set_result(prediction.cpu().numpy())\n                except Exception as e:\n                    for future in futures:\n                        future.set_exception(e)\n\nclass CachedInferenceService:\n    \"\"\"Inference with result caching\"\"\"\n    \n    def __init__(self, model, cache_size=1000, ttl_seconds=300):\n        self.model = model\n        self.cache = {}\n        self.cache_size = cache_size\n        self.ttl_seconds = ttl_seconds\n        self.hits = 0\n        self.misses = 0\n    \n    def _hash_input(self, input_data):\n        \"\"\"Generate cache key from input\"\"\"\n        if isinstance(input_data, torch.Tensor):\n            input_bytes = input_data.cpu().numpy().tobytes()\n        else:\n            input_bytes = np.array(input_data).tobytes()\n        return hashlib.md5(input_bytes).hexdigest()\n    \n    def predict(self, input_data):\n        \"\"\"Predict with caching\"\"\"\n        cache_key = self._hash_input(input_data)\n        \n        # Check cache\n        if cache_key in self.cache:\n            result, timestamp = self.cache[cache_key]\n            if time.time() - timestamp < self.ttl_seconds:\n                self.hits += 1\n                return result\n        \n        # Cache miss - run inference\n        self.misses += 1\n        with torch.no_grad():\n            result = self.model(input_data)\n        \n        # Update cache (LRU eviction)\n        if len(self.cache) >= self.cache_size:\n            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])\n            del self.cache[oldest_key]\n        \n        self.cache[cache_key] = (result, time.time())\n        return result\n    \n    def get_cache_stats(self):\n        total = self.hits + self.misses\n        hit_rate = self.hits / total if total > 0 else 0\n        return {\n            'hits': self.hits,\n            'misses': self.misses,\n            'hit_rate': hit_rate,\n            'cache_size': len(self.cache)\n        }\n\nclass OptimizedInferenceService:\n    \"\"\"Combined optimizations for real-time inference\"\"\"\n    \n    def __init__(self, model):\n        # Model optimization\n        self.model = self._optimize_model(model)\n        \n        # Caching\n        self.cached_service = CachedInferenceService(self.model)\n        \n        # Batching\n        self.batched_service = BatchedInferenceService(self.model)\n    \n    def _optimize_model(self, model):\n        \"\"\"Apply model optimizations\"\"\"\n        model.eval()\n        \n        # TorchScript compilation\n        example_input = torch.randn(1, 3, 224, 224)\n        traced_model = torch.jit.trace(model, example_input)\n        traced_model = torch.jit.optimize_for_inference(traced_model)\n        \n        return traced_model\n    \n    async def predict_async(self, input_data):\n        \"\"\"Async batched inference\"\"\"\n        return await self.batched_service.predict(input_data)\n    \n    def predict_sync(self, input_data):\n        \"\"\"Synchronous cached inference\"\"\"\n        return self.cached_service.predict(input_data)\n\n# Usage\nmodel = YourModel()\nservice = OptimizedInferenceService(model)\n\n# Sync inference with caching\ninput_data = torch.randn(1, 3, 224, 224)\nresult = service.predict_sync(input_data)\n\n# Async batched inference\nasync def main():\n    tasks = [\n        service.predict_async(torch.randn(1, 3, 224, 224))\n        for _ in range(100)\n    ]\n    results = await asyncio.gather(*tasks)\n    return results\n\nresults = asyncio.run(main())\nprint(f\"Processed {len(results)} requests\")\nprint(f\"Cache stats: {service.cached_service.get_cache_stats()}\")"
        }
      ]
    }
  ]
}
