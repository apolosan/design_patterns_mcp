{
  "id": "model-interpretation-bigdata",
  "name": "Model Interpretation for Big Data",
  "category": "Big Data Analysis",
  "description": "Techniques to explain and interpret machine learning models, ensuring transparency and accountability in big data applications",
  "when_to_use": "Regulatory compliance\nModel debugging\nFeature importance analysis\nBias detection\nStakeholder communication",
  "benefits": "Increases model trust\nIdentifies biases\nSupports regulatory compliance\nImproves model debugging\nEnhances decision-making",
  "drawbacks": "Computational overhead\nMay reduce accuracy\nComplex implementation\nLimited for some model types",
  "use_cases": "Financial risk assessment\nHealthcare diagnostics\nAutonomous systems\nCredit scoring\nLegal decision support",
  "complexity": "High",
  "tags": ["big-data", "model-interpretation", "explainability", "transparency", "bias-detection"],
  "examples": {
    "python": {
      "language": "python",
      "code": "import shap\nimport lime\nimport lime.lime_tabular\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\ndef model_interpretation_bigdata(model, X_train, X_test, y_train, y_test, method='shap'):\n    \"\"\"\n    Interpret machine learning models for big data applications\n    \n    Parameters:\n    - model: trained ML model\n    - X_train, X_test: training and test features\n    - y_train, y_test: training and test labels\n    - method: 'shap', 'lime', or 'permutation_importance'\n    \"\"\"\n    \n    if method == 'shap':\n        # SHAP (SHapley Additive exPlanations)\n        explainer = shap.TreeExplainer(model)\n        shap_values = explainer.shap_values(X_test)\n        \n        # Summary plot\n        plt.figure(figsize=(10, 6))\n        shap.summary_plot(shap_values, X_test, show=False)\n        plt.title('SHAP Feature Importance')\n        plt.tight_layout()\n        plt.show()\n        \n        # Waterfall plot for first prediction\n        plt.figure(figsize=(10, 6))\n        shap.plots.waterfall(explainer.expected_value[1], shap_values[1][0], X_test.iloc[0], show=False)\n        plt.title('SHAP Waterfall Plot - First Prediction')\n        plt.tight_layout()\n        plt.show()\n        \n        return {\n            'method': 'shap',\n            'shap_values': shap_values,\n            'feature_importance': np.abs(shap_values).mean(axis=0),\n            'explainer': explainer\n        }\n    \n    elif method == 'lime':\n        # LIME (Local Interpretable Model-agnostic Explanations)\n        lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n            X_train.values,\n            feature_names=X_train.columns,\n            class_names=['Negative', 'Positive'],\n            discretize_continuous=True\n        )\n        \n        # Explain first test instance\n        exp = lime_explainer.explain_instance(\n            X_test.iloc[0].values,\n            model.predict_proba,\n            num_features=10\n        )\n        \n        # Plot explanation\n        plt.figure(figsize=(10, 6))\n        exp.as_pyplot_figure()\n        plt.title('LIME Explanation - First Instance')\n        plt.tight_layout()\n        plt.show()\n        \n        return {\n            'method': 'lime',\n            'explanation': exp,\n            'lime_explainer': lime_explainer\n        }\n    \n    else:\n        # Permutation Feature Importance\n        from sklearn.inspection import permutation_importance\n        \n        perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n        \n        # Plot feature importance\n        plt.figure(figsize=(10, 6))\n        sorted_idx = perm_importance.importances_mean.argsort()\n        plt.barh(X_test.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n        plt.xlabel('Permutation Importance')\n        plt.title('Feature Importance (Permutation)')\n        plt.tight_layout()\n        plt.show()\n        \n        return {\n            'method': 'permutation_importance',\n            'importance_scores': perm_importance.importances_mean,\n            'feature_names': X_test.columns,\n            'std_scores': perm_importance.importances_std\n        }\n\n# Example usage\n# Generate sample data\nnp.random.seed(42)\nX = pd.DataFrame({\n    'feature_1': np.random.randn(1000),\n    'feature_2': np.random.randn(1000),\n    'feature_3': np.random.randn(1000),\n    'feature_4': np.random.randn(1000),\n    'feature_5': np.random.randn(1000)\n})\ny = (X['feature_1'] + X['feature_2'] * 2 + np.random.randn(1000) * 0.1 > 0).astype(int)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a simple model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Interpret the model using SHAP\ninterpretation_result = model_interpretation_bigdata(\n    model, X_train, X_test, y_train, y_test, method='shap'\n)\n\nprint(f\"Interpretation method: {interpretation_result['method']}\")\nprint(f\"Feature importance shape: {interpretation_result['feature_importance'].shape}\")\nprint(\"Feature importance (mean absolute SHAP values):\")\nfor name, importance in zip(X_test.columns, interpretation_result['feature_importance']):\n    print(f\"{name}: {importance:.4f}\")"
    }
  }
}
